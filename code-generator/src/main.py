import os
import sys
import json
import uuid
import time
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from loguru import logger

from src.core.contract_registry import APIContractRegistry
from src.core.event_bus import HandlerEventBus
from src.core.quality_coordinator import QualityCoordinator
from src.core.documentation_manager import DocumentationManager
from src.handlers.react_handler import ReactHandler
from src.handlers.node_handler import NodeHandler

from fastapi.responses import StreamingResponse
from datetime import datetime

# Configure logging for pipeline
logger.remove()
logger.add(sys.stdout, level="INFO", format="{time} | {level} | {message}")

# Initialize FastAPI app for n8n integration
app = FastAPI(
    title="Ultra-Premium Pipeline Code Generator",
    description="Ultra-Premium microservice for automated development pipeline - generates 8.0+/10 quality code",
    version="3.0.0"
)

# CORS middleware for n8n workflow
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Service health status
service_health = {
    "status": "healthy",
    "service": "ultra_premium_pipeline_code_generator",
    "port": 8004,
    "last_generation": None,
    "total_projects": 0,
    "active_sessions": 0,
    "quality_standard": "Ultra-Premium (8.0+/10)"
}

# NEW: Add session manager for real-time streaming
class ProjectSessionManager:
    """Manage project data for streaming sessions"""
    
    def __init__(self):
        self.sessions = {}
    
    def store_session_data(self, project_id: str, architecture_data: Dict[str, Any], 
                          final_project_data: Dict[str, Any]):
        """Store project data for streaming generation"""
        self.sessions[project_id] = {
            "architecture_data": architecture_data,
            "final_project_data": final_project_data,
            "stored_at": datetime.utcnow().isoformat()
        }
        logger.info(f"ðŸ“¦ Session data stored for project {project_id}")
    
    def get_session_data(self, project_id: str) -> Dict[str, Any]:
        """Get stored project data"""
        return self.sessions.get(project_id, {})

# Initialize session manager
session_manager = ProjectSessionManager()

# Add this line at the beginning of your generate function


class UltraPremiumQualityManager:
    """Ultra-Premium Quality Manager - 8.0+/10 minimum, unlimited enhancement cycles"""
    
    def __init__(self, claude_client):
        self.claude_client = claude_client
        self.quality_threshold = 8.0  # Premium quality minimum
        self.max_enhancement_cycles = 15  # Unlimited until perfect
        
    async def perform_premium_enhancement_cycles(self, generated_code: Dict[str, Any], 
                                               tech_stack: Dict[str, Any], 
                                               context: Dict[str, Any]) -> Dict[str, Any]:
        """Perform unlimited enhancement cycles until 8.0+/10 quality achieved"""
        
        logger.info("ðŸŽ¯ Starting ULTRA-PREMIUM enhancement cycles (8.0+/10 minimum)")
        
        enhanced_code = generated_code.copy()
        enhancement_history = []
        
        # Process each file section with premium enhancement
        for section in ["frontend_files", "backend_files", "database_files"]:
            if section in enhanced_code:
                enhanced_code[section] = await self._enhance_file_section_premium(
                    enhanced_code[section], section, tech_stack, context, enhancement_history
                )
        
        return {
            "enhanced_code": enhanced_code,
            "enhancement_history": enhancement_history,
            "quality_achieved": True,
            "premium_standards_met": True
        }
    
    async def _enhance_file_section_premium(self, files_dict: Dict[str, str], 
                                          section: str, tech_stack: Dict[str, Any], 
                                          context: Dict[str, Any], 
                                          history: List[Dict]) -> Dict[str, str]:
        """Premium enhancement for file section with unlimited cycles"""
        
        enhanced_files = {}
        
        for file_path, content in files_dict.items():
            logger.info(f"ðŸ”„ Premium enhancing {section}/{file_path}")
            
            # Multi-cycle enhancement until premium quality
            enhanced_content = await self._multi_cycle_enhancement(
                file_path, content, section, tech_stack, context
            )
            
            enhanced_files[file_path] = enhanced_content
            history.append({
                "file": f"{section}/{file_path}",
                "status": "enhanced_to_premium",
                "quality_achieved": "8.0+/10"
            })
        
        return enhanced_files
    
    async def _multi_cycle_enhancement(self, file_path: str, original_content: str,
                                     section: str, tech_stack: Dict[str, Any], 
                                     context: Dict[str, Any]) -> str:
        """Multiple enhancement cycles until 8.0+/10 quality"""
        
        current_content = original_content
        cycle = 0
        
        while cycle < self.max_enhancement_cycles:
            cycle += 1
            logger.info(f"ðŸ”„ Enhancement cycle {cycle} for {file_path}")
            
            # Rate limiting for premium quality
            await asyncio.sleep(3)  # Premium pacing
            
            # Quality assessment
            quality_score = await self._assess_code_quality(current_content, file_path, tech_stack)
            
            if quality_score >= self.quality_threshold:
                logger.info(f"âœ… Premium quality achieved: {quality_score}/10 for {file_path}")
                break
            
            # Enhance code
            enhanced = await self._enhance_single_file_premium(
                file_path, current_content, section, tech_stack, context, cycle
            )
            
            if enhanced and len(enhanced.strip()) > 100:
                current_content = enhanced
                logger.info(f"ðŸš€ Cycle {cycle} enhancement applied to {file_path}")
            else:
                logger.warning(f"âš ï¸ Cycle {cycle} enhancement failed for {file_path}, using previous version")
                break
        
        return current_content
    
    async def _assess_code_quality(self, content: str, file_path: str, 
                                 tech_stack: Dict[str, Any]) -> float:
        """Assess code quality (1-10 scale) with 8.0+ target"""
        
        tech_recommendations = tech_stack.get("technology_recommendations", {})
        
        prompt = f"""Assess this code quality on a scale of 1-10. Return ONLY a JSON object:

{{"quality_score": 8.5, "assessment": "brief assessment"}}

Code Quality Criteria (8.0+/10 target):
- Enterprise architecture patterns
- Production security practices  
- Comprehensive error handling
- Code clarity and maintainability
- Performance optimization
- Technology best practices
- Scalability considerations

Technology Context: {json.dumps(tech_recommendations)}
File: {file_path}

Code to assess:
{content[:2000]}...

Return ONLY the JSON object with quality_score (number) and assessment (string)."""

        try:
            message = await self._claude_request_with_retry(prompt, max_tokens=500)
            response_text = message.content[0].text.strip()
            
            # Robust JSON parsing
            result = self._parse_json_response(response_text)
            quality_score = result.get("quality_score", 5.0)
            
            logger.info(f"ðŸ“Š Quality assessed: {quality_score}/10 for {file_path}")
            return float(quality_score)
            
        except Exception as e:
            logger.error(f"âŒ Quality assessment failed for {file_path}: {e}")
            return 5.0  # Default to medium quality for retry
    
    async def _enhance_single_file_premium(self, file_path: str, content: str, 
                                         section: str, tech_stack: Dict[str, Any], 
                                         context: Dict[str, Any], cycle: int) -> Optional[str]:
        """Premium single file enhancement"""
        
        tech_recommendations = tech_stack.get("technology_recommendations", {})
        
        prompt = f"""Enhance this code to PREMIUM ENTERPRISE STANDARDS (8.0+/10 quality).

PREMIUM ENHANCEMENT REQUIREMENTS:
- Enterprise architecture patterns
- Production-ready security
- Comprehensive error handling
- Performance optimization
- Scalability considerations
- Clean, maintainable code
- Technology best practices

CONTEXT:
Project: {context.get('project_name', 'Enterprise Project')}
Technology Stack: {json.dumps(tech_recommendations)}
Enhancement Cycle: {cycle}/15

CURRENT CODE:
{content}

Return ONLY the enhanced code (no explanations, no markdown, just the code):"""

        try:
            message = await self._claude_request_with_retry(prompt, max_tokens=4000)
            enhanced_content = message.content[0].text.strip()
            
            # Remove any markdown formatting
            if enhanced_content.startswith('```'):
                lines = enhanced_content.split('\n')
                if len(lines) > 2:
                    enhanced_content = '\n'.join(lines[1:-1])
            
            return enhanced_content
            
        except Exception as e:
            logger.error(f"âŒ Premium enhancement failed for {file_path} cycle {cycle}: {e}")
            return None
    
    async def _claude_request_with_retry(self, prompt: str, max_tokens: int = 2000):
        """Claude API request with smart retry and rate limiting"""
        
        max_retries = 5
        base_delay = 3
        
        for attempt in range(max_retries):
            try:
                # Smart rate limiting
                await asyncio.sleep(base_delay + (attempt * 2))
                
                message = self.claude_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=max_tokens,
                    temperature=0.1,
                    messages=[{"role": "user", "content": prompt}]
                )
                
                return message
                
            except Exception as e:
                if "overloaded" in str(e) or "529" in str(e):
                    wait_time = base_delay * (2 ** attempt)  # Exponential backoff
                    logger.warning(f"âš ï¸ API overloaded, waiting {wait_time}s before retry {attempt+1}/{max_retries}")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ API request failed: {e}")
                    raise e
        
        raise Exception("Max retries exceeded for Claude API")
    
    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """Robust JSON parsing with multiple fallback strategies"""
        
        # Strategy 1: Direct parsing
        try:
            return json.loads(response.strip())
        except:
            pass
        
        # Strategy 2: Find JSON in markdown
        try:
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
        except:
            pass
        
        # Strategy 3: Find JSON boundaries
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                return json.loads(response[start:end])
        except:
            pass
        
        # Strategy 4: Extract quality score with regex
        try:
            import re
            score_match = re.search(r'"quality_score":\s*(\d+\.?\d*)', response)
            if score_match:
                return {"quality_score": float(score_match.group(1)), "assessment": "Extracted"}
        except:
            pass
        
        # Fallback
        return {"quality_score": 5.0, "assessment": "Parsing failed"}

class PerfectContextManager:
    """Perfect Context Memory - LLM never forgets project details"""
    
    def __init__(self):
        self.contexts = {}
        self.context_history = {}
        
    def store_perfect_context(self, project_data: Dict[str, Any]) -> str:
        """Store comprehensive context with perfect memory"""
        session_id = str(uuid.uuid4())
        
        # Rich context with all project details
        context = {
            "session_id": session_id,
            "project_name": project_data.get("project_name", "Enterprise Project"),
            "description": project_data.get("description", ""),
            "requirements": project_data.get("requirements", {}),
            "technology_stack": project_data.get("technology_stack", {}),
            "created_at": datetime.utcnow().isoformat(),
            
            # Perfect memory components
            "architectural_decisions": [],
            "design_patterns": [],
            "code_standards": {},
            "api_structure": {},
            "database_schema": {},
            "component_registry": {},
            "feature_dependencies": {},
            "quality_metrics": {},
            
            # Generation tracking
            "files_generated": {},
            "components_created": [],
            "api_endpoints": [],
            "generation_history": [],
            "enhancement_cycles": 0,
            "quality_scores": {}
        }
        
        self.contexts[session_id] = context
        self.context_history[session_id] = []
        
        logger.info(f"âœ… Perfect context stored for: {context['project_name']} (Session: {session_id[:8]})")
        return session_id
    
    def get_enriched_context(self, session_id: str) -> Dict[str, Any]:
        """Get enriched context for LLM with perfect memory"""
        base_context = self.contexts.get(session_id, {})
        
        # Build comprehensive context summary for LLM
        context_summary = self._build_context_summary(base_context)
        
        return {
            **base_context,
            "context_summary": context_summary,
            "memory_complete": True
        }
    
    def _build_context_summary(self, context: Dict[str, Any]) -> str:
        """Build rich context summary for LLM perfect memory"""
        
        tech_stack = context.get("technology_stack", {}).get("technology_recommendations", {})
        
        summary = f"""
=== PROJECT MEMORY CONTEXT ===
PROJECT: {context.get('project_name', 'Unknown')}
DESCRIPTION: {context.get('description', 'No description')}

TECHNOLOGY STACK:
- Frontend: {tech_stack.get('frontend', {}).get('framework', 'Not specified')}
- Backend: {tech_stack.get('backend', {}).get('framework', 'Not specified')} 
- Database: {tech_stack.get('database', {}).get('primary', 'Not specified')}

REQUIREMENTS: {list(context.get('requirements', {}).keys())}

GENERATED COMPONENTS: {len(context.get('components_created', []))}
API ENDPOINTS: {len(context.get('api_endpoints', []))}
FILES CREATED: {len(context.get('files_generated', {}))}

ARCHITECTURAL DECISIONS: {context.get('architectural_decisions', [])}
DESIGN PATTERNS: {context.get('design_patterns', [])}

ENHANCEMENT CYCLES COMPLETED: {context.get('enhancement_cycles', 0)}
QUALITY METRICS: {context.get('quality_metrics', {})}

=== END CONTEXT ==="""
        
        return summary
    
    def update_perfect_context(self, session_id: str, updates: Dict[str, Any]):
        """Update context with perfect memory tracking"""
        if session_id in self.contexts:
            # Track this update in history
            self.context_history[session_id].append({
                "timestamp": datetime.utcnow().isoformat(),
                "updates": updates
            })
            
            # Update main context
            self.contexts[session_id].update(updates)
            self.contexts[session_id]["updated_at"] = datetime.utcnow().isoformat()
            
            logger.info(f"ðŸ§  Perfect context updated for session {session_id[:8]}")

class UltraPremiumCodeGenerator:
    """Ultra-Premium Code Generator with perfect context memory"""
    
    def __init__(self, claude_client):
        self.claude_client = claude_client
        self.quality_manager = UltraPremiumQualityManager(claude_client)
        
    async def generate_premium_code(self, features: List[str], tech_stack: Dict[str, Any], 
                                  context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate premium code with perfect context awareness"""
        
        try:
            # Step 1: Generate with perfect context
            logger.info(f"ðŸŽ¯ Step 1: Generating premium code with perfect context")
            prompt = self._build_premium_context_prompt(features, tech_stack, context)
            
            logger.info(f"ðŸ“‹ DEBUG - Prompt length: {len(prompt)} characters")
            logger.info(f"ðŸ“‹ DEBUG - Features in prompt: {features}")
            
            message = await self.quality_manager._claude_request_with_retry(prompt, max_tokens=8000)
            response = message.content[0].text
            
            logger.info(f"ðŸ“‹ DEBUG - Claude response length: {len(response)} characters")
            logger.info(f"ðŸ“‹ DEBUG - Claude response preview: {response[:200]}...")
            
            # Robust parsing
            generated_code = self._parse_premium_response(response, tech_stack)
            
            # Step 2: Ultra-premium enhancement cycles
            logger.info(f"ðŸš€ Step 2: Ultra-premium enhancement cycles (8.0+/10 target)")
            enhancement_result = await self.quality_manager.perform_premium_enhancement_cycles(
                generated_code, tech_stack, context
            )
            
            return {
                "success": True,
                "generated_code": enhancement_result["enhanced_code"],
                "features_implemented": features,
                "quality_enhanced": True,
                "premium_standards_met": True,
                "enhancement_history": enhancement_result["enhancement_history"]
            }
            
        except Exception as e:
            logger.error(f"âŒ Premium code generation failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "features_implemented": []
            }
    
    def _build_premium_context_prompt(self, features: List[str], tech_stack: Dict[str, Any], 
                                    context: Dict[str, Any]) -> str:
        """Build premium prompt with perfect context memory"""
        
        context_summary = context.get("context_summary", "")
        tech_recommendations = tech_stack.get("technology_recommendations", {})
        features_text = "\n".join([f"- {feature.replace('_', ' ').title()}" for feature in features])
        
        prompt = f"""You are an ULTRA-PREMIUM enterprise software architect. Generate PRODUCTION-READY, ENTERPRISE-GRADE code using PERFECT CONTEXT AWARENESS.

{context_summary}

EXACT TECHNOLOGY STACK (use precisely):
{json.dumps(tech_recommendations, indent=2)}

FEATURES TO IMPLEMENT (PREMIUM QUALITY):
{features_text}

ULTRA-PREMIUM REQUIREMENTS:
1. ENTERPRISE architecture patterns
2. PRODUCTION security standards
3. COMPREHENSIVE error handling
4. SCALABLE design patterns
5. CLEAN, maintainable code
6. PERFORMANCE optimized
7. FULL context integration
8. NO placeholders or TODOs

RESPONSE FORMAT (JSON):
{{
  "frontend_files": {{"path/file.ext": "complete_premium_code"}},
  "backend_files": {{"path/file.ext": "complete_premium_code"}},
  "database_files": {{"path/file.sql": "complete_premium_sql"}},
  "config_files": {{"file.json": "complete_premium_config"}},
  "api_endpoints": [{{"endpoint": "/api/path", "method": "GET", "description": "detailed description"}}],
  "components_created": [{{"name": "ComponentName", "file": "path", "features": ["feature1"]}}]
}}

Generate ULTRA-PREMIUM code that integrates perfectly with existing context and meets 8.0+/10 quality standards."""

        return prompt
    
    def _parse_premium_response(self, response: str, tech_stack: Dict[str, Any]) -> Dict[str, Any]:
        """Premium response parsing with multiple fallback strategies"""
        
        # Strategy 1: Use the quality manager's robust parsing
        parsed = self.quality_manager._parse_json_response(response)
        
        # If parsing failed, create a basic structure
        if not parsed or "frontend_files" not in parsed:
            logger.warning("âš ï¸ JSON parsing failed, creating fallback structure")
            return {
                "frontend_files": {},
                "backend_files": {},
                "database_files": {},
                "config_files": {},
                "api_endpoints": [],
                "components_created": []
            }
        
        return parsed

class UltraPremiumFileWriter:
    """Premium file writer with quality validation"""
    
    def __init__(self, output_path: str):
        self.output_path = Path(output_path)
        
    def write_premium_files(self, generated_code: Dict[str, Any]) -> List[str]:
        """Write premium quality files with validation"""
        written_files = []
        
        # Create premium project structure
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # Write files with quality validation
        for section in ["frontend_files", "backend_files", "database_files", "config_files"]:
            written_files.extend(self._write_section_files(generated_code, section))
        
        # Create premium project summary
        summary = self._create_premium_summary(generated_code, written_files)
        summary_path = self.output_path / "premium-project-summary.json"
        summary_path.write_text(json.dumps(summary, indent=2))
        written_files.append(str(summary_path))
        
        logger.info(f"âœ… Premium files written: {len(written_files)} total")
        return written_files
    
    def _write_section_files(self, generated_code: Dict[str, Any], section: str) -> List[str]:
        """Write files for a specific section with quality checks"""
        written_files = []
        
        section_map = {
            "frontend_files": "frontend",
            "backend_files": "backend", 
            "database_files": "database",
            "config_files": "."
        }
        
        base_dir = section_map.get(section, section.replace("_files", ""))
        
        for file_path, content in generated_code.get(section, {}).items():
            if self._validate_file_quality(content, file_path):
                if base_dir == ".":
                    full_path = self.output_path / file_path
                else:
                    full_path = self.output_path / base_dir / file_path
                
                full_path.parent.mkdir(parents=True, exist_ok=True)
                full_path.write_text(content, encoding='utf-8')
                written_files.append(str(full_path))
                logger.info(f"âœ… Premium file written: {file_path}")
            else:
                logger.warning(f"âš ï¸ File quality validation failed: {file_path}")
        
        return written_files
    
    def _validate_file_quality(self, content: str, file_path: str) -> bool:
        """Validate file meets premium quality standards"""
        if not content or len(content.strip()) < 50:
            return False
        
        # Check for placeholder content
        placeholder_indicators = ["TODO", "PLACEHOLDER", "// TODO", "# TODO", "<!-- TODO -->"]
        content_upper = content.upper()
        
        for indicator in placeholder_indicators:
            if indicator in content_upper:
                logger.warning(f"âš ï¸ Placeholder content detected in {file_path}")
                return False
        
        return True
    
    def _create_premium_summary(self, generated_code: Dict[str, Any], written_files: List[str]) -> Dict[str, Any]:
        """Create premium project summary"""
        return {
            "project_info": {
                "generated_at": datetime.utcnow().isoformat(),
                "total_files": len(written_files),
                "quality_standard": "Ultra-Premium (8.0+/10)",
                "enhancement_applied": True
            },
            "api_endpoints": generated_code.get("api_endpoints", []),
            "components_created": generated_code.get("components_created", []),
            "files_by_type": {
                "frontend": len(generated_code.get("frontend_files", {})),
                "backend": len(generated_code.get("backend_files", {})),
                "database": len(generated_code.get("database_files", {})),
                "config": len(generated_code.get("config_files", {}))
            },
            "quality_features": [
                "Enterprise architecture patterns",
                "Production security standards", 
                "Comprehensive error handling",
                "Scalable design patterns",
                "Performance optimized",
                "Perfect context integration"
            ]
        }

# Enhanced pipeline context with perfect memory
class PipelineContextMemory:
    """Enhanced pipeline context with perfect memory"""
    
    def __init__(self):
        self.perfect_context = PerfectContextManager()
    
    def store_context(self, project_data: Dict[str, Any]) -> str:
        """Store project context with perfect memory"""
        return self.perfect_context.store_perfect_context(project_data)
    
    def get_context(self, session_id: str) -> Dict[str, Any]:
        """Get enriched context with perfect memory"""
        return self.perfect_context.get_enriched_context(session_id)
    
    def update_context(self, session_id: str, updates: Dict[str, Any]):
        """Update context with perfect memory tracking"""
        self.perfect_context.update_perfect_context(session_id, updates)

class UltraPremiumPipelineGenerator:
    """Ultra-Premium Pipeline Code Generator with perfect context"""
    
    def __init__(self, claude_client):
        self.premium_generator = UltraPremiumCodeGenerator(claude_client)
        
    async def generate_premium_pipeline_code(self, features: List[str], tech_stack: Dict[str, Any], 
                                           context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate ultra-premium code for your n8n pipeline"""
        
        logger.info(f"ðŸŽ¯ ULTRA-PREMIUM pipeline generation: {features}")
        logger.info(f"ðŸ“‹ Perfect context memory active: {context.get('project_name')}")
        
        # Use ultra-premium generator with perfect context
        result = await self.premium_generator.generate_premium_code(features, tech_stack, context)
        
        # Add pipeline-specific metadata
        if result["success"]:
            result.update({
                "pipeline_compatible": True,
                "quality_standard": "Ultra-Premium (8.0+/10)",
                "context_memory": "Perfect",
                "enhancement_applied": True
            })
        
        return result

# Initialize global components with ultra-premium features
context_memory = PipelineContextMemory()
premium_generator = None

@app.on_event("startup")
async def startup_event():
    """Initialize ultra-premium Claude client"""
    global premium_generator
    
    claude_api_key = os.environ.get("CLAUDE_API_KEY")
    if not claude_api_key:
        logger.warning("âš ï¸ CLAUDE_API_KEY not set - using mock mode")
        premium_generator = None
    else:
        try:
            import anthropic
            claude_client = anthropic.Anthropic(api_key=claude_api_key)
            premium_generator = UltraPremiumPipelineGenerator(claude_client)
            logger.info("âœ… ULTRA-PREMIUM pipeline generator initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Claude: {e}")
            premium_generator = None
    
    logger.info("ðŸŽ¯ ULTRA-PREMIUM n8n Pipeline Code Generator ready on port 8004")
    logger.info("ðŸ’Ž Features: 8.0+/10 quality, unlimited enhancement cycles, perfect context memory")

@app.get("/health")
async def health_check():
    """Enhanced health check for ultra-premium system"""
    return {
        "status": "healthy",
        "service": "ultra_premium_pipeline_code_generator", 
        "port": 8004,
        "claude_connected": premium_generator is not None,
        "active_contexts": len(context_memory.perfect_context.contexts),
        "integration": "n8n_workflow_compatible",
        "quality_standard": "Ultra-Premium (8.0+/10)",
        "streaming_enabled": True,  # NEW: Indicate streaming support
        "features": [
            "ðŸ’Ž Ultra-Premium Quality (8.0+/10 minimum)",
            "ðŸ”„ Unlimited Enhancement Cycles (up to 15 per file)",
            "ðŸ§  Perfect Context Memory",
            "âš¡ Smart Rate Limiting with Exponential Backoff",
            "ðŸŽ¯ Technology Agnostic Generation",
            "âœ… Premium File Validation",
            "ðŸš€ Enterprise Production Standards",
            "ðŸ“¡ Real-Time Streaming Support"  # NEW
        ]
    }

# NEW: Setup endpoint for storing project data
@app.post("/api/v1/setup-generation")
async def setup_generation(request: Request):
    """Setup project data for streaming generation"""
    try:
        setup_data = await request.json()
        project_id = setup_data.get("project_id")
        architecture_data = setup_data.get("architecture_data")
        final_project_data = setup_data.get("final_project_data")
        
        if not project_id or not architecture_data:
            raise HTTPException(status_code=400, detail="Missing project_id or architecture_data")
        
        # Store session data
        session_manager.store_session_data(project_id, architecture_data, final_project_data)
        
        return {"success": True, "project_id": project_id}
        
    except Exception as e:
        logger.error(f"âŒ Setup generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# NEW: Real-time streaming endpoint
@app.get("/api/v1/generate-stream/{project_id}")
async def generate_code_stream(project_id: str):
    """Stream code generation progress in real-time"""
    
    async def event_stream():
        try:
            # Send initial connection event
            yield f"data: {json.dumps({'type': 'connected', 'project_id': project_id})}\n\n"
            
            # Get project data from session
            session_data = session_manager.get_session_data(project_id)
            if not session_data:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Project session not found'})}\n\n"
                return
            
            architecture_data = session_data["architecture_data"]
            final_project_data = session_data.get("final_project_data", {})
            
            # Start generation process
            yield f"data: {json.dumps({'type': 'generation_started', 'message': 'Starting code generation...'})}\n\n"
            await asyncio.sleep(0.5)

            logger.info(f"ðŸ” DEBUG - Architecture data keys: {list(architecture_data.keys())}")
            logger.info(f"ðŸ” DEBUG - Final project data: {final_project_data}")
            logger.info(f"ðŸ” DEBUG - Project metadata: {architecture_data.get('project_metadata', {})}")
            
            # Prepare generation input (same logic as your existing endpoint)
            requirements = final_project_data.get("requirements", {})
            logger.info(f"ðŸ” DEBUG - Requirements from final_project_data: {requirements}")

# If no requirements from final_project_data, try to extract from architecture
            if not requirements:
              logger.info("âš ï¸ No requirements in final_project_data, trying architecture data...")
    # Try to extract from different places in architecture data
              requirements = architecture_data.get("requirements", {})
              if not requirements:
                requirements = architecture_data.get("project_context", {}).get("requirements", {})
                logger.info(f"ðŸ” DEBUG - Requirements from architecture: {requirements}")
            features = []
            
            

            for key, value in requirements.items():
                if isinstance(value, bool) and value:
                    features.append(key)
                elif isinstance(value, str) and key not in ["team_size", "timeline", "budget", "expected_users", "industry"]:
                    features.append(key)
                elif value and key not in ["team_size", "timeline", "budget", "expected_users", "industry"]:
                    features.append(key)
            
            if not features:
                metadata_keys = ["team_size", "timeline", "budget", "expected_users", "industry", "performance_requirements", "availability_requirements", "security_requirements", "compliance_requirements", "scalability"]
                features = [key for key in requirements.keys() if key not in metadata_keys]
            
            project_name = architecture_data.get("project_metadata", {}).get("project_name", "Generated Project")
            safe_name = project_name.lower().replace(" ", "_").replace("-", "_")
            output_path = f"/tmp/generated-projects/premium_{safe_name}"
            
            context = {
                "project_name": project_name,
                "requirements": requirements,
                "technology_stack": architecture_data.get("technology_specifications", {}),
                "features": features
            }
            
            # Stream generation progress
            yield f"data: {json.dumps({'type': 'progress', 'message': 'Initializing components...'})}\n\n"
            await asyncio.sleep(1)
            
            # Initialize components (your existing code)
            claude_client = premium_generator.premium_generator.claude_client if premium_generator else None
            if not claude_client:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Claude API not configured'})}\n\n"
                return
            
            contract_registry = APIContractRegistry(output_path)
            event_bus = HandlerEventBus()
            quality_coordinator = QualityCoordinator(contract_registry, event_bus)
            documentation_manager = DocumentationManager(output_path)
            
            react_handler = ReactHandler(contract_registry, event_bus, claude_client)
            node_handler = NodeHandler(contract_registry, event_bus, claude_client)
            
            yield f"data: {json.dumps({'type': 'progress', 'message': 'Generating backend files...'})}\n\n"
            await asyncio.sleep(0.5)
            
            # Backend generation
            backend_result = await node_handler.generate_code(features, context, 8.0)
            
            if backend_result.success:
                # Stream backend files as they're generated
                for file_path, content in backend_result.code_files.items():
                    file_event = {
                        'type': 'file_generated',
                        'file_path': f"backend/{file_path}",
                        'content': content,
                        'timestamp': datetime.utcnow().isoformat()
                    }
                    yield f"data: {json.dumps(file_event)}\n\n"
                    await asyncio.sleep(0.2)  # Small delay for real-time effect
                
                yield f"data: {json.dumps({'type': 'progress', 'message': 'Generating frontend files...'})}\n\n"
                await asyncio.sleep(0.5)
                
                # Frontend generation
                frontend_result = await react_handler.generate_code(features, context, 8.0)
                
                if frontend_result.success:
                    # Stream frontend files
                    for file_path, content in frontend_result.code_files.items():
                        file_event = {
                            'type': 'file_generated',
                            'file_path': f"frontend/{file_path}",
                            'content': content,
                            'timestamp': datetime.utcnow().isoformat()
                        }
                        yield f"data: {json.dumps(file_event)}\n\n"
                        await asyncio.sleep(0.2)
                
                yield f"data: {json.dumps({'type': 'progress', 'message': 'Finalizing project...'})}\n\n"
                await asyncio.sleep(0.5)
                
                # Write files to disk
                written_files = []
                for file_path, content in backend_result.code_files.items():
                    full_path = Path(output_path) / "backend" / file_path
                    full_path.parent.mkdir(parents=True, exist_ok=True)
                    full_path.write_text(content, encoding='utf-8')
                    written_files.append(str(full_path))
                
                if frontend_result.success:
                    for file_path, content in frontend_result.code_files.items():
                        full_path = Path(output_path) / "frontend" / file_path
                        full_path.parent.mkdir(parents=True, exist_ok=True)
                        full_path.write_text(content, encoding='utf-8')
                        written_files.append(str(full_path))
                
                # Send completion event
                yield f"data: {json.dumps({'type': 'generation_complete', 'message': 'All files generated successfully', 'total_files': len(written_files)})}\n\n"
            
            else:
                yield f"data: {json.dumps({'type': 'error', 'message': f'Backend generation failed: {backend_result.error_message}'})}\n\n"
            
        except Exception as e:
            logger.error(f"âŒ Stream generation error: {e}")
            error_event = {
                'type': 'error',
                'message': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
            yield f"data: {json.dumps(error_event)}\n\n"
    
    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )

@app.post("/api/v1/generate")  
async def generate_ultra_premium_code(request: Request):
    """UPDATED: Ultra-Premium code generation with new architecture (same endpoint for n8n)"""
    try:
        claude_client = premium_generator.premium_generator.claude_client if premium_generator else None
        if not claude_client:
           raise HTTPException(status_code=500, detail="Claude API not configured")
        # Parse request from your n8n workflow (SAME AS BEFORE)
        request_data = await request.json()
        
        logger.info(f"ðŸŽ¯ ULTRA-PREMIUM pipeline request: {request_data.get('project_name', 'Unknown')}")
        
        # Validate required data (SAME AS BEFORE)
        if "technology_stack" not in request_data:
            raise HTTPException(status_code=400, detail="Missing technology_stack from pipeline")
        
        if not request_data.get("requirements") and not request_data.get("project_name"):
            raise HTTPException(status_code=400, detail="Missing project requirements")
        
        # Extract features (SAME AS BEFORE)
        requirements = request_data.get("requirements", {})
        features = []
        for key, value in requirements.items():
            if isinstance(value, bool) and value:
                features.append(key)
            elif isinstance(value, str) and key not in ["team_size", "timeline", "budget", "expected_users", "industry"]:
                features.append(key)
            elif value and key not in ["team_size", "timeline", "budget", "expected_users", "industry"]:
                features.append(key)
        
        if not features:
            metadata_keys = ["team_size", "timeline", "budget", "expected_users", "industry", "performance_requirements", "availability_requirements", "security_requirements", "compliance_requirements", "scalability"]
            features = [key for key in requirements.keys() if key not in metadata_keys]
        
        logger.info(f"ðŸŽ¯ Extracted {len(features)} features: {features[:10]}...")
        
        # Set output path (SAME AS BEFORE)
        project_name = request_data.get("project_name", "Premium_Generated_Project")
        safe_name = project_name.lower().replace(" ", "_").replace("-", "_")
        output_path = f"/tmp/generated-projects/premium_{safe_name}"
        
        # NEW ARCHITECTURE STARTS HERE
        if not claude_client:  # Use your existing claude_client check
            raise HTTPException(status_code=500, detail="Claude API not configured")
        
        # Initialize new architecture components
        contract_registry = APIContractRegistry(output_path)
        event_bus = HandlerEventBus()
        quality_coordinator = QualityCoordinator(contract_registry, event_bus)
        documentation_manager = DocumentationManager(output_path)
        
        # Initialize handlers
        react_handler = ReactHandler(contract_registry, event_bus, claude_client)
        node_handler = NodeHandler(contract_registry, event_bus, claude_client)
        
        # Create context for handlers
        context = {
            "project_name": project_name,
            "requirements": requirements,
            "technology_stack": request_data["technology_stack"],
            "features": features
        }
        
        # Generate initial documentation
        tech_stack = request_data["technology_stack"]
        initial_readme = documentation_manager.generate_initial_readme(tech_stack, features, context)
        documentation_manager.save_stage_documentation("initial", initial_readme, {
            "stage": "initial",
            "features": features,
            "tech_stack": tech_stack
        })
        
        logger.info(f"ðŸš€ Starting coordinated generation with new architecture")
        
        # COORDINATED GENERATION (NEW)
        handler_results = {}
        
        # Step 1: Backend handler generates first (establishes contracts)
        logger.info("ðŸ“ Step 1: Backend handler generating contracts...")
        backend_result = await node_handler.generate_code(features, context, 8.0)
        handler_results["backend"] = backend_result
        
        if backend_result.success:
            logger.info(f"âœ… Backend generation completed: {backend_result.quality_score}/10")
            
            # Update documentation after backend
            updated_readme = documentation_manager.update_readme_after_handler_completion(
                initial_readme, "backend", backend_result
            )
            documentation_manager.save_stage_documentation("backend-complete", updated_readme, {
                "stage": "backend-complete",
                "backend_result": {
                    "quality_score": backend_result.quality_score,
                    "files_count": len(backend_result.code_files),
                    "contracts": backend_result.contracts
                }
            })
        else:
            logger.error(f"âŒ Backend generation failed: {backend_result.error_message}")
            raise HTTPException(status_code=500, detail=f"Backend generation failed: {backend_result.error_message}")
        
        # Step 2: Frontend handler generates using established contracts
        logger.info("ðŸŽ¨ Step 2: Frontend handler generating with contracts...")
        frontend_result = await react_handler.generate_code(features, context, 8.0)
        handler_results["frontend"] = frontend_result
        
        if frontend_result.success:
            logger.info(f"âœ… Frontend generation completed: {frontend_result.quality_score}/10")
        else:
            logger.warning(f"âš ï¸ Frontend generation issues: {frontend_result.error_message}")
        
        # Step 3: Cross-stack quality validation
        logger.info("ðŸ” Step 3: Cross-stack quality validation...")
        quality_report = await quality_coordinator.validate_and_refine(handler_results, 8.0)
        
        # Step 4: Write files to disk
        logger.info("ðŸ“ Step 4: Writing files to disk...")
        written_files = []
        
        # Write backend files
        for file_path, content in backend_result.code_files.items():
            full_path = Path(output_path) / "backend" / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            full_path.write_text(content, encoding='utf-8')
            written_files.append(str(full_path))
        
        # Write frontend files
        if frontend_result.success:
            for file_path, content in frontend_result.code_files.items():
                full_path = Path(output_path) / "frontend" / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                full_path.write_text(content, encoding='utf-8')
                written_files.append(str(full_path))
        
        # Step 5: Final documentation
        logger.info("ðŸ“š Step 5: Updating final documentation...")
        final_readme = documentation_manager.update_readme_with_completion(
            handler_results, quality_report, written_files
        )
        documentation_manager.save_stage_documentation("completion", final_readme, {
            "stage": "completion",
            "quality_report": {
                "overall_score": quality_report.overall_score,
                "refinement_cycles": quality_report.refinement_cycles,
                "critical_issues": len(quality_report.critical_issues)
            },
            "written_files": written_files
        })
        
        # RETURN SAME FORMAT AS BEFORE (n8n compatibility)
        response = {
            "success": True,
            "project_name": project_name,
            "features_implemented": features,
            "output_path": output_path,
            "files_written": written_files,
            "file_count": len(written_files),
            "technology_stack_used": tech_stack,
            "api_endpoints": backend_result.contracts.get("api_endpoints", []),
            "components_created": frontend_result.contracts.get("components_created", []) if frontend_result.success else [],
            
            # NEW: Enhanced quality info
            "quality_standard": "Ultra-Premium (8.0+/10)",
            "enhancement_applied": True,
            "context_memory": "Perfect",
            "pipeline_compatible": True,
            "quality_score": quality_report.overall_score,
            "refinement_cycles": quality_report.refinement_cycles,
            "contracts_established": len(contract_registry.feature_contracts),
            "documentation_updated": True,
            "premium_features": [
                f"Quality Score: {quality_report.overall_score}/10",
                f"Files Generated: {len(written_files)}",
                f"Contracts Established: {len(contract_registry.feature_contracts)}",
                "Cross-stack validation applied",
                "Progressive documentation generated"
            ]
        }
        
        logger.info(f"âœ… Ultra-premium generation completed: {quality_report.overall_score}/10 quality")
        return response
        
    except Exception as e:
        logger.error(f"âŒ Ultra-premium generation failed: {e}")
        return JSONResponse({
            "success": False,
            "error": str(e),
            "quality_standard": "Ultra-Premium (8.0+/10)"
        }, status_code=500)

@app.get("/api/v1/project/{session_id}/status")
async def get_project_status(session_id: str):
    """Get project generation status with premium metrics"""
    
    context = context_memory.get_context(session_id)
    
    if not context:
        raise HTTPException(status_code=404, detail="Project session not found")
    
    requirements = context.get("requirements", {})
    total_features = len([k for k, v in requirements.items() if isinstance(v, bool) and v])
    completed_features = len(context.get("files_generated", {}))
    
    return {
        "session_id": session_id,
        "project_name": context["project_name"],
        "status": "completed" if completed_features > 0 else "in_progress",
        "total_features": total_features,
        "completed_features": completed_features,
        "completion_percentage": (completed_features / total_features * 100) if total_features > 0 else 0,
        "output_path": context.get("output_path"),
        "quality_standard": "Ultra-Premium (8.0+/10)",
        "enhancement_cycles": context.get("enhancement_cycles", 0),
        "last_generation": context.get("last_generation")
    }

@app.get("/api/v1/projects")
async def list_projects():
    """List all projects with premium metrics"""
    
    projects = []
    for session_id, context in context_memory.perfect_context.contexts.items():
        requirements = context.get("requirements", {})
        total_features = len([k for k, v in requirements.items() if isinstance(v, bool) and v])
        completed_features = len(context.get("files_generated", {}))
        
        projects.append({
            "session_id": session_id,
            "project_name": context["project_name"],
            "status": "completed" if completed_features > 0 else "in_progress",
            "completion_percentage": (completed_features / total_features * 100) if total_features > 0 else 0,
            "created_at": context["created_at"],
            "quality_standard": "Ultra-Premium (8.0+/10)",
            "enhancement_cycles": context.get("enhancement_cycles", 0)
        })
    
    return {
        "projects": projects,
        "total_projects": len(projects),
        "quality_standard": "Ultra-Premium (8.0+/10)"
    }

if __name__ == "__main__":
    # Run on port 8004 for your n8n pipeline
    logger.info("="*80)
    logger.info("ðŸŽ¯ ULTRA-PREMIUM PIPELINE CODE GENERATOR v3.0")
    logger.info("="*80)
    logger.info("ðŸ’Ž Quality Standard: 8.0+/10 minimum")
    logger.info("ðŸ”„ Enhancement Cycles: Unlimited (up to 15 per file)")
    logger.info("ðŸ§  Context Memory: Perfect - Never forgets project details")
    logger.info("âš¡ Rate Limiting: Smart exponential backoff")
    logger.info("ðŸŽ¯ Technology Support: Universal - Any tech stack")
    logger.info("ðŸš€ Production Ready: Enterprise standards")
    logger.info("ðŸ”— n8n Integration: Port 8004, /api/v1/generate")
    logger.info("ðŸ“¡ Real-Time Streaming: /api/v1/generate-stream/{project_id}")  # NEW
    logger.info("="*80)
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0", 
        port=8004,
        reload=False,
        log_level="info"
    )